{
  "title": "Vision & Representation Experiments",
  "slug": "vision-representation",
  "kind": "experiment",
  "year": "2024",
  "role": "Researcher – model design and analysis",
  "themes": [
    "Vision & Representation"
  ],
  "tags": [
    "efficientnet",
    "vision-transformer",
    "resnet",
    "representation-learning"
  ],
  "heroMetric": "Comparative experiments across modern vision backbones",
  "hook": "Course and lab experiments exploring modern vision backbones and representation learning.",
  "featured": false,
  "order": "7\r",
  "body": {
    "raw": "\r\n\r\n## Overview\r\n\r\nA set of supervised and self-supervised style experiments executed as part of a deep learning curriculum and self-directed study. The goal was to compare architectures (EfficientNet, ViT, custom ResNet) under controlled augmentation and compute budgets, and to explore interpretability methods (Grad-CAM) and perceptual loss variants for downstream image tasks. :contentReference[oaicite:9]{index=9}\r\n\r\n## Representative Experiments\r\n\r\n- **EfficientNetV2 fine-tuning:** Fine-tuned on Oxford Flowers-102 with custom augmentations and multi-GPU training; validated class-balance strategies and augmentation schedules.\r\n- **ViT & Representational Probes:** Trained ViT-B32 on a domain-specific PV-fault dataset, compared head-only vs full-finetune methods.\r\n- **ResNet-36 & custom activations:** Implemented a ResNet-36 from scratch, experimented with an analytic custom activation and benchmarked against ReLU baselines.\r\n- **Interpretability:** Generated Grad-CAM maps to inspect class discriminative regions and evaluate whether features align with human-perceptible cues.\r\n\r\n## Outcomes\r\n\r\n- Demonstrated practical knowledge of end-to-end vision pipelines (dataset curation → augmentation → multi-GPU training → evaluation).\r\n- Gained fluency in model selection trade-offs, fine-tuning strategies, and basic interpretability techniques useful for downstream research and production prototypes. :contentReference[oaicite:10]{index=10}\r\n\r\n## Notes\r\n\r\nThese experiments are compact and reproducible; they serve as methodological building blocks for larger perception projects (e.g., Mesquite MoCap visual fusion, Happenstance).\r\n",
    "code": "var Component=(()=>{var p=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var x=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var i in e)a(t,i,{get:e[i],enumerable:!0})},o=(t,e,i,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of h(e))!g.call(t,r)&&r!==i&&a(t,r,{get:()=>e[r],enumerable:!(s=m(e,r))||s.enumerable});return t};var b=(t,e,i)=>(i=t!=null?p(f(t)):{},o(e||!t||!t.__esModule?a(i,\"default\",{value:t,enumerable:!0}):i,t)),y=t=>o(a({},\"__esModule\",{value:!0}),t);var l=x((M,c)=>{c.exports=_jsx_runtime});var R={};v(R,{default:()=>u,frontmatter:()=>w});var n=b(l()),w={title:\"Vision & Representation Experiments\",slug:\"vision-representation\",kind:\"experiment\",year:\"2024\",role:\"Researcher \\u2013 model design and analysis\",themes:[\"Vision & Representation\"],tags:[\"efficientnet\",\"vision-transformer\",\"resnet\",\"representation-learning\"],heroMetric:\"Comparative experiments across modern vision backbones\",hook:\"Course and lab experiments exploring modern vision backbones and representation learning.\",featured:!1,order:7};function d(t){let e={h2:\"h2\",li:\"li\",p:\"p\",strong:\"strong\",ul:\"ul\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Overview\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"A set of supervised and self-supervised style experiments executed as part of a deep learning curriculum and self-directed study. The goal was to compare architectures (EfficientNet, ViT, custom ResNet) under controlled augmentation and compute budgets, and to explore interpretability methods (Grad-CAM) and perceptual loss variants for downstream image tasks. :contentReference[oaicite:9]\",index=9]}),`\n`,(0,n.jsx)(e.h2,{children:\"Representative Experiments\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"EfficientNetV2 fine-tuning:\"}),\" Fine-tuned on Oxford Flowers-102 with custom augmentations and multi-GPU training; validated class-balance strategies and augmentation schedules.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"ViT & Representational Probes:\"}),\" Trained ViT-B32 on a domain-specific PV-fault dataset, compared head-only vs full-finetune methods.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"ResNet-36 & custom activations:\"}),\" Implemented a ResNet-36 from scratch, experimented with an analytic custom activation and benchmarked against ReLU baselines.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Interpretability:\"}),\" Generated Grad-CAM maps to inspect class discriminative regions and evaluate whether features align with human-perceptible cues.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Outcomes\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Demonstrated practical knowledge of end-to-end vision pipelines (dataset curation \\u2192 augmentation \\u2192 multi-GPU training \\u2192 evaluation).\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Gained fluency in model selection trade-offs, fine-tuning strategies, and basic interpretability techniques useful for downstream research and production prototypes. :contentReference[oaicite:10]\",index=10]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Notes\"}),`\n`,(0,n.jsx)(e.p,{children:\"These experiments are compact and reproducible; they serve as methodological building blocks for larger perception projects (e.g., Mesquite MoCap visual fusion, Happenstance).\"})]})}function u(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(d,{...t})}):d(t)}return y(R);})();\n;return Component;"
  },
  "_id": "projects/vision-representation.mdx",
  "_raw": {
    "sourceFilePath": "projects/vision-representation.mdx",
    "sourceFileName": "vision-representation.mdx",
    "sourceFileDir": "projects",
    "contentType": "mdx",
    "flattenedPath": "projects/vision-representation"
  },
  "type": "Project",
  "url": "/projects/vision-representation"
}