{
  "title": "Temporal Fusion Transformers for Financial Forecasting",
  "slug": "tft-finance",
  "kind": "experiment",
  "year": "2024",
  "role": "Co-lead â€“ modeling and data pipeline",
  "themes": [
    "Temporal Models & Sequences"
  ],
  "tags": [
    "time-series",
    "temporal-fusion-transformer",
    "finance",
    "deep-learning"
  ],
  "heroMetric": "Exploratory forecasting on S&P 500 and related series",
  "hook": "Experiments with Temporal Fusion Transformers on financial time-series data.",
  "featured": false,
  "order": 5,
  "gallery": [],
  "links": {
    "github": "https://github.com/samufrank/sp500-tft-forecasting",
    "type": "ProjectLinks",
    "_raw": {}
  },
  "body": {
    "raw": "\r\n\r\n## Project Context\r\n\r\nDeveloped as the final project for a graduate Deep Learning course, this effort applied Temporal Fusion Transformers (TFT) to S&P 500 forecasting using a mixed-frequency pipeline that combined daily price data with lower-frequency macroeconomic indicators. The goal was to investigate whether TFT's attention and gating mechanisms improve multi-horizon forecasting relative to LSTM baselines and to explore interpretability via attention weights. \r\n\r\n## Approach & System Overview\r\n\r\n- **Data pipeline:** Collected daily S&P prices (Yahoo Finance) and monthly/quarterly macro indicators (FRED). Designed alignment and imputation to handle mixed frequencies and avoid lookahead bias.\r\n- **Models implemented:** TFT (primary) and LSTM (baseline) implemented in PyTorch Lightning; training on GPU with time-series cross-validation splits.\r\n- **Architectural twist:** Experimented with separate embeddings for endogenous (price series) vs. exogenous (macro indicators) inputs to improve feature specialization.\r\n\r\n## Key Technical Points\r\n\r\n- Carefully handled time alignment: for each forecast horizon we limited use of \"known future\" features to only those legitimately available at forecast time.\r\n- Used attention weight visualizations to surface which inputs the model relied on for different horizons (e.g., interest rates for mid-horizon).\r\n- Logged experiments and results with TensorBoard and structured model checkpoints.\r\n\r\n## Results & Takeaways\r\n\r\n- TFT provided modest improvements (~5% RMSE improvement over tuned LSTM baselines) in the tested windows; TFT training incurred higher compute/time costs but offered improved interpretability via attention heatmaps.\r\n- Ablation showed that removing macroeconomic exogenous features increased error noticeably (~~10% in some splits), suggesting their value for multi-horizon forecasting.\r\n- These results are course-level exploratory findings and illustrate competence with modern temporal architectures and disciplined experimental practice in constrained timelines. :contentReference[oaicite:8]{index=8}\r\n\r\n## My Contribution\r\n\r\n- Led the data engineering: multi-source ingestion, alignment, and preprocessing.\r\n- Implemented the TFT and LSTM training loops, evaluation metrics, and ablation experiments.\r\n- Produced attention visualizations and draft report/presentation for the course.\r\n\r\n## Next Steps\r\n\r\n- Extend the setup to include higher-frequency real-time indicators, perform more robust cross-validation, and explore physically informed priors for improved generalization in volatile regimes.\r\n",
    "code": "var Component=(()=>{var m=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var T=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),v=(i,e)=>{for(var t in e)o(i,t,{get:e[t],enumerable:!0})},s=(i,e,t,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!f.call(i,r)&&r!==t&&o(i,r,{get:()=>e[r],enumerable:!(a=p(e,r))||a.enumerable});return i};var x=(i,e,t)=>(t=i!=null?m(g(i)):{},s(e||!i||!i.__esModule?o(t,\"default\",{value:i,enumerable:!0}):t,i)),y=i=>s(o({},\"__esModule\",{value:!0}),i);var d=T((M,l)=>{l.exports=_jsx_runtime});var b={};v(b,{default:()=>h,frontmatter:()=>w});var n=x(d()),w={title:\"Temporal Fusion Transformers for Financial Forecasting\",slug:\"tft-finance\",kind:\"experiment\",year:\"2024\",role:\"Co-lead \\u2013 modeling and data pipeline\",themes:[\"Temporal Models & Sequences\"],tags:[\"time-series\",\"temporal-fusion-transformer\",\"finance\",\"deep-learning\"],heroMetric:\"Exploratory forecasting on S&P 500 and related series\",hook:\"Experiments with Temporal Fusion Transformers on financial time-series data.\",featured:!1,order:5,links:{github:\"https://github.com/samufrank/sp500-tft-forecasting\"}};function c(i){let e={h2:\"h2\",li:\"li\",p:\"p\",strong:\"strong\",ul:\"ul\",...i.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Project Context\"}),`\n`,(0,n.jsx)(e.p,{children:\"Developed as the final project for a graduate Deep Learning course, this effort applied Temporal Fusion Transformers (TFT) to S&P 500 forecasting using a mixed-frequency pipeline that combined daily price data with lower-frequency macroeconomic indicators. The goal was to investigate whether TFT's attention and gating mechanisms improve multi-horizon forecasting relative to LSTM baselines and to explore interpretability via attention weights.\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Approach & System Overview\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Data pipeline:\"}),\" Collected daily S&P prices (Yahoo Finance) and monthly/quarterly macro indicators (FRED). Designed alignment and imputation to handle mixed frequencies and avoid lookahead bias.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Models implemented:\"}),\" TFT (primary) and LSTM (baseline) implemented in PyTorch Lightning; training on GPU with time-series cross-validation splits.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Architectural twist:\"}),\" Experimented with separate embeddings for endogenous (price series) vs. exogenous (macro indicators) inputs to improve feature specialization.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Key Technical Points\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:'Carefully handled time alignment: for each forecast horizon we limited use of \"known future\" features to only those legitimately available at forecast time.'}),`\n`,(0,n.jsx)(e.li,{children:\"Used attention weight visualizations to surface which inputs the model relied on for different horizons (e.g., interest rates for mid-horizon).\"}),`\n`,(0,n.jsx)(e.li,{children:\"Logged experiments and results with TensorBoard and structured model checkpoints.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Results & Takeaways\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"TFT provided modest improvements (~5% RMSE improvement over tuned LSTM baselines) in the tested windows; TFT training incurred higher compute/time costs but offered improved interpretability via attention heatmaps.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Ablation showed that removing macroeconomic exogenous features increased error noticeably (~~10% in some splits), suggesting their value for multi-horizon forecasting.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"These results are course-level exploratory findings and illustrate competence with modern temporal architectures and disciplined experimental practice in constrained timelines. :contentReference[oaicite:8]\",index=8]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"My Contribution\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Led the data engineering: multi-source ingestion, alignment, and preprocessing.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Implemented the TFT and LSTM training loops, evaluation metrics, and ablation experiments.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Produced attention visualizations and draft report/presentation for the course.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Next Steps\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Extend the setup to include higher-frequency real-time indicators, perform more robust cross-validation, and explore physically informed priors for improved generalization in volatile regimes.\"}),`\n`]})]})}function h(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,{...i,children:(0,n.jsx)(c,{...i})}):c(i)}return y(b);})();\n;return Component;"
  },
  "_id": "projects/tft-finance.mdx",
  "_raw": {
    "sourceFilePath": "projects/tft-finance.mdx",
    "sourceFileName": "tft-finance.mdx",
    "sourceFileDir": "projects",
    "contentType": "mdx",
    "flattenedPath": "projects/tft-finance"
  },
  "type": "Project",
  "url": "/projects/tft-finance"
}