{
  "title": "Multi-Phone 3D Capture Rig",
  "slug": "multi-phone-3d-rig",
  "kind": "research",
  "year": "2024–2025",
  "role": "Designer & developer – rig, calibration, and reconstruction",
  "themes": [
    "Low-Cost Motion Capture & XR Tools"
  ],
  "tags": [
    "multi-view",
    "volumetric-capture",
    "smartphone",
    "3D-reconstruction"
  ],
  "heroMetric": "4–8 synchronized phones · small-scale volumetric capture",
  "hook": "A flexible, low-cost smartphone rig for experimental multi-view 3D capture.",
  "featured": false,
  "order": 2,
  "links": {
    "github": "https://github.com/Danyal-0-1/phone-carousel",
    "type": "ProjectLinks",
    "_raw": {}
  },
  "body": {
    "raw": "\r\n\r\n## Problem & Motivation\r\n\r\nHigh-quality volumetric capture typically requires expensive cameras and infrastructure. The Multi-Phone Rig explores how widely available smartphones can be synchronized and calibrated into an affordable multi-view capture platform for motion research, XR content creation, and embodied interaction studies. The aim is not to beat studio systems on raw accuracy but to provide a practical, cheap, and transportable rig suitable for prototyping and fieldwork. :contentReference[oaicite:5]{index=5}\r\n\r\n## Approach & System Overview\r\n\r\n- **Physical rig:** A modular mount that holds 4–8 phones with consistent poses and line-of-sight to the capture volume.\r\n- **Sync mechanism:** A Raspberry Pi sends a UDP countdown to companion apps on each phone; phones trigger capture at the appointed timestamp and stream frames to a local server.\r\n- **Calibration:** Use ArUco markers for initial extrinsic calibration of each phone camera (solvePnP → absolute camera poses).\r\n- **Processing pipeline:** Per-frame 2D keypoints (MediaPipe / OpenPose) → temporal buffering → triangulation for 3D skeletons; optional COLMAP/CUSTOM pipeline for static mesh reconstruction.\r\n\r\n## Implementation Notes\r\n\r\n- The sync protocol prioritizes low-latency UDP control for start-of-capture with lightweight timestamp negotiation and small buffering to accommodate jitter.\r\n- Calibration obtains per-camera intrinsic/extrinsic parameters using OpenCV utilities; calibration is stored and reused across sessions to reduce setup time.\r\n- For dynamic capture, a short buffer allows minor alignment adjustments (frame-shift compensation) to reduce temporal misalignment effects.\r\n\r\n## Evaluation & Results\r\n\r\n- Early tests captured walking and gesturing sequences with **synchronization jitter < 50 ms**; in practice we observed ~20–50 ms depending on Wi-Fi conditions.\r\n- Fusion of 2D keypoints into 3D skeletons produced visually plausible motion suitable for animation and downstream XR use. Mesh reconstructions for static poses were coarse but usable for rapid prototyping.\r\n- The system is a pragmatic tool for rapid capture and exploratory HRI experiments; quantitative accuracy vs. optical mocap is left for future formal benchmarking. :contentReference[oaicite:6]{index=6}\r\n\r\n## My Contribution\r\n\r\n- Designed the rig and the app/server sync architecture.\r\n- Implemented the UDP synchronization protocol and the capture orchestration scripts.\r\n- Built the calibration pipeline (ArUco-based) and the initial 3D fusion code.\r\n\r\n## Outcomes & Next Steps\r\n\r\n- Immediate next steps: hardware trigger integration for sub-10 ms sync, increased camera counts, and systematic benchmarking against Mesquite MoCap and an optical system to quantify spatial/temporal error.\r\n",
    "code": "var Component=(()=>{var p=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var i in e)o(t,i,{get:e[i],enumerable:!0})},s=(t,e,i,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!f.call(t,r)&&r!==i&&o(t,r,{get:()=>e[r],enumerable:!(a=u(e,r))||a.enumerable});return t};var w=(t,e,i)=>(i=t!=null?p(g(t)):{},s(e||!t||!t.__esModule?o(i,\"default\",{value:t,enumerable:!0}):i,t)),v=t=>s(o({},\"__esModule\",{value:!0}),t);var l=b((P,c)=>{c.exports=_jsx_runtime});var D={};y(D,{default:()=>d,frontmatter:()=>x});var n=w(l()),x={title:\"Multi-Phone 3D Capture Rig\",slug:\"multi-phone-3d-rig\",kind:\"research\",year:\"2024\\u20132025\",role:\"Designer & developer \\u2013 rig, calibration, and reconstruction\",themes:[\"Low-Cost Motion Capture & XR Tools\"],tags:[\"multi-view\",\"volumetric-capture\",\"smartphone\",\"3D-reconstruction\"],heroMetric:\"4\\u20138 synchronized phones \\xB7 small-scale volumetric capture\",hook:\"A flexible, low-cost smartphone rig for experimental multi-view 3D capture.\",featured:!1,order:2,links:{github:\"https://github.com/Danyal-0-1/phone-carousel\"}};function h(t){let e={h2:\"h2\",li:\"li\",p:\"p\",strong:\"strong\",ul:\"ul\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Problem & Motivation\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"High-quality volumetric capture typically requires expensive cameras and infrastructure. The Multi-Phone Rig explores how widely available smartphones can be synchronized and calibrated into an affordable multi-view capture platform for motion research, XR content creation, and embodied interaction studies. The aim is not to beat studio systems on raw accuracy but to provide a practical, cheap, and transportable rig suitable for prototyping and fieldwork. :contentReference[oaicite:5]\",index=5]}),`\n`,(0,n.jsx)(e.h2,{children:\"Approach & System Overview\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Physical rig:\"}),\" A modular mount that holds 4\\u20138 phones with consistent poses and line-of-sight to the capture volume.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Sync mechanism:\"}),\" A Raspberry Pi sends a UDP countdown to companion apps on each phone; phones trigger capture at the appointed timestamp and stream frames to a local server.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Calibration:\"}),\" Use ArUco markers for initial extrinsic calibration of each phone camera (solvePnP \\u2192 absolute camera poses).\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Processing pipeline:\"}),\" Per-frame 2D keypoints (MediaPipe / OpenPose) \\u2192 temporal buffering \\u2192 triangulation for 3D skeletons; optional COLMAP/CUSTOM pipeline for static mesh reconstruction.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Implementation Notes\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"The sync protocol prioritizes low-latency UDP control for start-of-capture with lightweight timestamp negotiation and small buffering to accommodate jitter.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Calibration obtains per-camera intrinsic/extrinsic parameters using OpenCV utilities; calibration is stored and reused across sessions to reduce setup time.\"}),`\n`,(0,n.jsx)(e.li,{children:\"For dynamic capture, a short buffer allows minor alignment adjustments (frame-shift compensation) to reduce temporal misalignment effects.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Evaluation & Results\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Early tests captured walking and gesturing sequences with \",(0,n.jsx)(e.strong,{children:\"synchronization jitter < 50 ms\"}),\"; in practice we observed ~20\\u201350 ms depending on Wi-Fi conditions.\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Fusion of 2D keypoints into 3D skeletons produced visually plausible motion suitable for animation and downstream XR use. Mesh reconstructions for static poses were coarse but usable for rapid prototyping.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"The system is a pragmatic tool for rapid capture and exploratory HRI experiments; quantitative accuracy vs. optical mocap is left for future formal benchmarking. :contentReference[oaicite:6]\",index=6]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"My Contribution\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Designed the rig and the app/server sync architecture.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Implemented the UDP synchronization protocol and the capture orchestration scripts.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Built the calibration pipeline (ArUco-based) and the initial 3D fusion code.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Outcomes & Next Steps\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Immediate next steps: hardware trigger integration for sub-10 ms sync, increased camera counts, and systematic benchmarking against Mesquite MoCap and an optical system to quantify spatial/temporal error.\"}),`\n`]})]})}function d(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(h,{...t})}):h(t)}return v(D);})();\n;return Component;"
  },
  "_id": "projects/multi-phone-3d-rig.mdx",
  "_raw": {
    "sourceFilePath": "projects/multi-phone-3d-rig.mdx",
    "sourceFileName": "multi-phone-3d-rig.mdx",
    "sourceFileDir": "projects",
    "contentType": "mdx",
    "flattenedPath": "projects/multi-phone-3d-rig"
  },
  "type": "Project",
  "url": "/projects/multi-phone-3d-rig"
}