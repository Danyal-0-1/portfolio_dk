{
  "cacheItemsMap": {
    "projects/mesquite-mocap.mdx": {
      "document": {
        "title": "Mesquite MoCap – Open-Source Wearable 6-DoF Motion Capture",
        "slug": "mesquite-mocap",
        "kind": "research",
        "year": "2024–2025",
        "role": "Co-developer – firmware, sensor fusion, benchmarking",
        "themes": [
          "Embodied Perception & Sensor Fusion",
          "Low-Cost Motion Capture & XR Tools"
        ],
        "tags": [
          "sensor-fusion",
          "embedded-systems",
          "motion-capture",
          "ESP32-C3",
          "XR"
        ],
        "heroMetric": "~32 FPS • <15 ms latency • ~99.7% packet delivery",
        "hook": "Open-source, wireless full-body 6-DoF motion capture using low-cost IMU nodes.",
        "featured": true,
        "order": 1,
        "links": {
          "github": "https://github.com/Mesquite-Mocap/mesquite.cc",
          "paper": "https://arxiv.org/abs/2512.22690",
          "acceptance": "https://fossunited.org/c/indiafoss/2025/cfp/8o6jom3fk9",
          "type": "ProjectLinks",
          "_raw": {}
        },
        "body": {
          "raw": "\r\n## Problem & Context\r\n\r\nCommercial motion-capture systems such as OptiTrack are accurate but expensive and bound to controlled\r\noptical setups. For XR, embodied interaction, and robotics research, we wanted a low-cost wearable\r\nalternative that could run in ordinary spaces and survive packet loss, drift, and network noise.\r\n\r\n## Approach & System Overview\r\n\r\nMesquite MoCap is a full-body suit built from 15 wireless ESP32-C3 IMU nodes worn across the body.\r\nEach node streams timestamped inertial data over Wi-Fi to a receiver, which fuses orientations and\r\nexports BVH skeletons for visualization and downstream use.\r\n\r\nAt a high level:\r\n\r\n- **Hardware:** 15 ESP32-C3 IMU modules (accelerometer + gyroscope) mounted on straps.\r\n- **Firmware:** Arduino-based code for synchronized streaming, buffering, and retry logic.\r\n- **Host pipeline:** Calibration, sensor fusion, inverse kinematics, and BVH export.\r\n- **Visualization:** Real-time 3D skeleton in Unity / WebXR.\r\n\r\n## Robustness & Evaluation\r\n\r\nWe tuned the sensor-fusion and networking stack to:\r\n\r\n- Sustain roughly 32 FPS streaming from all 15 nodes.\r\n- Keep end-to-end latency under 15 ms in typical lab conditions.\r\n- Maintain packet delivery around 99–100% over Wi-Fi.\r\n\r\nWe benchmarked the system against an OptiTrack setup, observing joint-angle error in a small-degree\r\nrange on key joints for walking and running sequences.\r\n\r\n## My Contribution\r\n\r\n- Co-developed and debugged the ESP32-C3 firmware, including synchronization and buffering.\r\n- Implemented parts of the sensor-fusion stack and helped tune filters and calibration procedures.\r\n- Worked on the BVH export path and host-side tools used for benchmarking and visualization.\r\n- Helped design experiments and interpret results for a paper now under review.\r\n\r\n## Outcomes & Next Steps\r\n\r\nMesquite demonstrates that affordable, open-source IMU-based mocap can approach optical quality under\r\nreal-time constraints. Future work includes integrating additional sensing modalities (e.g., depth\r\ncameras, environmental sensors) and using the suit as a platform for embodied AI and human–robot\r\ninteraction studies.\r\n",
          "code": "var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(r,e)=>()=>(e||r((e={exports:{}}).exports,e),e.exports),y=(r,e)=>{for(var i in e)o(r,i,{get:e[i],enumerable:!0})},a=(r,e,i,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of m(e))!f.call(r,t)&&t!==i&&o(r,t,{get:()=>e[t],enumerable:!(s=p(e,t))||s.enumerable});return r};var k=(r,e,i)=>(i=r!=null?h(g(r)):{},a(e||!r||!r.__esModule?o(i,\"default\",{value:r,enumerable:!0}):i,r)),w=r=>a(o({},\"__esModule\",{value:!0}),r);var l=b((C,d)=>{d.exports=_jsx_runtime});var M={};y(M,{default:()=>u,frontmatter:()=>v});var n=k(l()),v={title:\"Mesquite MoCap \\u2013 Open-Source Wearable 6-DoF Motion Capture\",slug:\"mesquite-mocap\",kind:\"research\",year:\"2024\\u20132025\",role:\"Co-developer \\u2013 firmware, sensor fusion, benchmarking\",themes:[\"Embodied Perception & Sensor Fusion\",\"Low-Cost Motion Capture & XR Tools\"],tags:[\"sensor-fusion\",\"embedded-systems\",\"motion-capture\",\"ESP32-C3\",\"XR\"],heroMetric:\"~32 FPS \\u2022 <15 ms latency \\u2022 ~99.7% packet delivery\",hook:\"Open-source, wireless full-body 6-DoF motion capture using low-cost IMU nodes.\",featured:!0,order:1,links:{github:\"https://github.com/Mesquite-Mocap/mesquite.cc\",paper:\"https://arxiv.org/abs/2512.22690\",acceptance:\"https://fossunited.org/c/indiafoss/2025/cfp/8o6jom3fk9\"}};function c(r){let e={h2:\"h2\",li:\"li\",p:\"p\",strong:\"strong\",ul:\"ul\",...r.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Problem & Context\"}),`\n`,(0,n.jsx)(e.p,{children:`Commercial motion-capture systems such as OptiTrack are accurate but expensive and bound to controlled\\r\noptical setups. For XR, embodied interaction, and robotics research, we wanted a low-cost wearable\\r\nalternative that could run in ordinary spaces and survive packet loss, drift, and network noise.`}),`\n`,(0,n.jsx)(e.h2,{children:\"Approach & System Overview\"}),`\n`,(0,n.jsx)(e.p,{children:`Mesquite MoCap is a full-body suit built from 15 wireless ESP32-C3 IMU nodes worn across the body.\\r\nEach node streams timestamped inertial data over Wi-Fi to a receiver, which fuses orientations and\\r\nexports BVH skeletons for visualization and downstream use.`}),`\n`,(0,n.jsx)(e.p,{children:\"At a high level:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Hardware:\"}),\" 15 ESP32-C3 IMU modules (accelerometer + gyroscope) mounted on straps.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Firmware:\"}),\" Arduino-based code for synchronized streaming, buffering, and retry logic.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Host pipeline:\"}),\" Calibration, sensor fusion, inverse kinematics, and BVH export.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Visualization:\"}),\" Real-time 3D skeleton in Unity / WebXR.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Robustness & Evaluation\"}),`\n`,(0,n.jsx)(e.p,{children:\"We tuned the sensor-fusion and networking stack to:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Sustain roughly 32 FPS streaming from all 15 nodes.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Keep end-to-end latency under 15 ms in typical lab conditions.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Maintain packet delivery around 99\\u2013100% over Wi-Fi.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:`We benchmarked the system against an OptiTrack setup, observing joint-angle error in a small-degree\\r\nrange on key joints for walking and running sequences.`}),`\n`,(0,n.jsx)(e.h2,{children:\"My Contribution\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Co-developed and debugged the ESP32-C3 firmware, including synchronization and buffering.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Implemented parts of the sensor-fusion stack and helped tune filters and calibration procedures.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Worked on the BVH export path and host-side tools used for benchmarking and visualization.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Helped design experiments and interpret results for a paper now under review.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Outcomes & Next Steps\"}),`\n`,(0,n.jsx)(e.p,{children:`Mesquite demonstrates that affordable, open-source IMU-based mocap can approach optical quality under\\r\nreal-time constraints. Future work includes integrating additional sensing modalities (e.g., depth\\r\ncameras, environmental sensors) and using the suit as a platform for embodied AI and human\\u2013robot\\r\ninteraction studies.`})]})}function u(r={}){let{wrapper:e}=r.components||{};return e?(0,n.jsx)(e,{...r,children:(0,n.jsx)(c,{...r})}):c(r)}return w(M);})();\n;return Component;"
        },
        "_id": "projects/mesquite-mocap.mdx",
        "_raw": {
          "sourceFilePath": "projects/mesquite-mocap.mdx",
          "sourceFileName": "mesquite-mocap.mdx",
          "sourceFileDir": "projects",
          "contentType": "mdx",
          "flattenedPath": "projects/mesquite-mocap"
        },
        "type": "Project",
        "url": "/projects/mesquite-mocap"
      },
      "documentHash": "1767592063294",
      "hasWarnings": false,
      "documentTypeName": "Project"
    },
    "projects/multi-phone-3d-rig.mdx": {
      "document": {
        "title": "Multi-Phone 3D Capture Rig",
        "slug": "multi-phone-3d-rig",
        "kind": "research",
        "year": "2024–2025",
        "role": "Designer & developer – rig, calibration, and reconstruction",
        "themes": [
          "Low-Cost Motion Capture & XR Tools"
        ],
        "tags": [
          "multi-view",
          "volumetric-capture",
          "smartphone",
          "3D-reconstruction"
        ],
        "heroMetric": "4–8 synchronized phones · small-scale volumetric capture",
        "hook": "A flexible, low-cost smartphone rig for experimental multi-view 3D capture.",
        "featured": false,
        "order": 2,
        "links": {
          "github": "https://github.com/Danyal-0-1/phone-carousel",
          "type": "ProjectLinks",
          "_raw": {}
        },
        "body": {
          "raw": "\r\n\r\n## Problem & Motivation\r\n\r\nHigh-quality volumetric capture typically requires expensive cameras and infrastructure. The Multi-Phone Rig explores how widely available smartphones can be synchronized and calibrated into an affordable multi-view capture platform for motion research, XR content creation, and embodied interaction studies. The aim is not to beat studio systems on raw accuracy but to provide a practical, cheap, and transportable rig suitable for prototyping and fieldwork. :contentReference[oaicite:5]{index=5}\r\n\r\n## Approach & System Overview\r\n\r\n- **Physical rig:** A modular mount that holds 4–8 phones with consistent poses and line-of-sight to the capture volume.\r\n- **Sync mechanism:** A Raspberry Pi sends a UDP countdown to companion apps on each phone; phones trigger capture at the appointed timestamp and stream frames to a local server.\r\n- **Calibration:** Use ArUco markers for initial extrinsic calibration of each phone camera (solvePnP → absolute camera poses).\r\n- **Processing pipeline:** Per-frame 2D keypoints (MediaPipe / OpenPose) → temporal buffering → triangulation for 3D skeletons; optional COLMAP/CUSTOM pipeline for static mesh reconstruction.\r\n\r\n## Implementation Notes\r\n\r\n- The sync protocol prioritizes low-latency UDP control for start-of-capture with lightweight timestamp negotiation and small buffering to accommodate jitter.\r\n- Calibration obtains per-camera intrinsic/extrinsic parameters using OpenCV utilities; calibration is stored and reused across sessions to reduce setup time.\r\n- For dynamic capture, a short buffer allows minor alignment adjustments (frame-shift compensation) to reduce temporal misalignment effects.\r\n\r\n## Evaluation & Results\r\n\r\n- Early tests captured walking and gesturing sequences with **synchronization jitter < 50 ms**; in practice we observed ~20–50 ms depending on Wi-Fi conditions.\r\n- Fusion of 2D keypoints into 3D skeletons produced visually plausible motion suitable for animation and downstream XR use. Mesh reconstructions for static poses were coarse but usable for rapid prototyping.\r\n- The system is a pragmatic tool for rapid capture and exploratory HRI experiments; quantitative accuracy vs. optical mocap is left for future formal benchmarking. :contentReference[oaicite:6]{index=6}\r\n\r\n## My Contribution\r\n\r\n- Designed the rig and the app/server sync architecture.\r\n- Implemented the UDP synchronization protocol and the capture orchestration scripts.\r\n- Built the calibration pipeline (ArUco-based) and the initial 3D fusion code.\r\n\r\n## Outcomes & Next Steps\r\n\r\n- Immediate next steps: hardware trigger integration for sub-10 ms sync, increased camera counts, and systematic benchmarking against Mesquite MoCap and an optical system to quantify spatial/temporal error.\r\n",
          "code": "var Component=(()=>{var p=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var i in e)o(t,i,{get:e[i],enumerable:!0})},s=(t,e,i,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!f.call(t,r)&&r!==i&&o(t,r,{get:()=>e[r],enumerable:!(a=u(e,r))||a.enumerable});return t};var w=(t,e,i)=>(i=t!=null?p(g(t)):{},s(e||!t||!t.__esModule?o(i,\"default\",{value:t,enumerable:!0}):i,t)),v=t=>s(o({},\"__esModule\",{value:!0}),t);var l=b((P,c)=>{c.exports=_jsx_runtime});var D={};y(D,{default:()=>d,frontmatter:()=>x});var n=w(l()),x={title:\"Multi-Phone 3D Capture Rig\",slug:\"multi-phone-3d-rig\",kind:\"research\",year:\"2024\\u20132025\",role:\"Designer & developer \\u2013 rig, calibration, and reconstruction\",themes:[\"Low-Cost Motion Capture & XR Tools\"],tags:[\"multi-view\",\"volumetric-capture\",\"smartphone\",\"3D-reconstruction\"],heroMetric:\"4\\u20138 synchronized phones \\xB7 small-scale volumetric capture\",hook:\"A flexible, low-cost smartphone rig for experimental multi-view 3D capture.\",featured:!1,order:2,links:{github:\"https://github.com/Danyal-0-1/phone-carousel\"}};function h(t){let e={h2:\"h2\",li:\"li\",p:\"p\",strong:\"strong\",ul:\"ul\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Problem & Motivation\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"High-quality volumetric capture typically requires expensive cameras and infrastructure. The Multi-Phone Rig explores how widely available smartphones can be synchronized and calibrated into an affordable multi-view capture platform for motion research, XR content creation, and embodied interaction studies. The aim is not to beat studio systems on raw accuracy but to provide a practical, cheap, and transportable rig suitable for prototyping and fieldwork. :contentReference[oaicite:5]\",index=5]}),`\n`,(0,n.jsx)(e.h2,{children:\"Approach & System Overview\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Physical rig:\"}),\" A modular mount that holds 4\\u20138 phones with consistent poses and line-of-sight to the capture volume.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Sync mechanism:\"}),\" A Raspberry Pi sends a UDP countdown to companion apps on each phone; phones trigger capture at the appointed timestamp and stream frames to a local server.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Calibration:\"}),\" Use ArUco markers for initial extrinsic calibration of each phone camera (solvePnP \\u2192 absolute camera poses).\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Processing pipeline:\"}),\" Per-frame 2D keypoints (MediaPipe / OpenPose) \\u2192 temporal buffering \\u2192 triangulation for 3D skeletons; optional COLMAP/CUSTOM pipeline for static mesh reconstruction.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Implementation Notes\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"The sync protocol prioritizes low-latency UDP control for start-of-capture with lightweight timestamp negotiation and small buffering to accommodate jitter.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Calibration obtains per-camera intrinsic/extrinsic parameters using OpenCV utilities; calibration is stored and reused across sessions to reduce setup time.\"}),`\n`,(0,n.jsx)(e.li,{children:\"For dynamic capture, a short buffer allows minor alignment adjustments (frame-shift compensation) to reduce temporal misalignment effects.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Evaluation & Results\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Early tests captured walking and gesturing sequences with \",(0,n.jsx)(e.strong,{children:\"synchronization jitter < 50 ms\"}),\"; in practice we observed ~20\\u201350 ms depending on Wi-Fi conditions.\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Fusion of 2D keypoints into 3D skeletons produced visually plausible motion suitable for animation and downstream XR use. Mesh reconstructions for static poses were coarse but usable for rapid prototyping.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"The system is a pragmatic tool for rapid capture and exploratory HRI experiments; quantitative accuracy vs. optical mocap is left for future formal benchmarking. :contentReference[oaicite:6]\",index=6]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"My Contribution\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Designed the rig and the app/server sync architecture.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Implemented the UDP synchronization protocol and the capture orchestration scripts.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Built the calibration pipeline (ArUco-based) and the initial 3D fusion code.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Outcomes & Next Steps\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Immediate next steps: hardware trigger integration for sub-10 ms sync, increased camera counts, and systematic benchmarking against Mesquite MoCap and an optical system to quantify spatial/temporal error.\"}),`\n`]})]})}function d(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(h,{...t})}):h(t)}return v(D);})();\n;return Component;"
        },
        "_id": "projects/multi-phone-3d-rig.mdx",
        "_raw": {
          "sourceFilePath": "projects/multi-phone-3d-rig.mdx",
          "sourceFileName": "multi-phone-3d-rig.mdx",
          "sourceFileDir": "projects",
          "contentType": "mdx",
          "flattenedPath": "projects/multi-phone-3d-rig"
        },
        "type": "Project",
        "url": "/projects/multi-phone-3d-rig"
      },
      "documentHash": "1767592068020",
      "hasWarnings": false,
      "documentTypeName": "Project"
    },
    "projects/opuntia.mdx": {
      "document": {
        "title": "Opuntia – Solar-Powered Environmental Sensing Station",
        "slug": "opuntia",
        "kind": "research",
        "year": "2024",
        "role": "Designer & builder – hardware, sensing, and logging",
        "themes": [
          "Embodied Perception & Sensor Fusion"
        ],
        "tags": [
          "IoT",
          "ESP32-C3",
          "environmental-sensing",
          "solar-power"
        ],
        "heroMetric": "Solar-powered · multi-sensor telemetry",
        "hook": "A long-running environmental station for logging and visualizing environmental change.",
        "featured": false,
        "order": 3,
        "links": {
          "github": "https://github.com/opuntia-cc",
          "type": "ProjectLinks",
          "_raw": {}
        },
        "body": {
          "raw": "\r\n\r\n## Problem & Motivation\r\n\r\nLong-term ecological sensing and sensor-driven art installations are often constrained by power, communications reliability, and timestamp integrity in outdoor deployments. Opuntia was built to demonstrate that a compact, solar-powered station can reliably collect, backfill, and serve environmental telemetry (temperature, humidity, pressure, soil moisture, UV) to a web dashboard for weeks without human intervention. This project was developed during an IoT course as a practical research prototype. :contentReference[oaicite:3]{index=3}\r\n\r\n## Approach & System Overview\r\n\r\nOpuntia emphasizes robustness and autonomy rather than experimental novelty: hardware and software are designed around a strict power budget, reliable timekeeping, and outage recovery.\r\n\r\n- **Hardware:** ESP32-C3 microcontroller, BME280 (temp/humidity/pressure), soil-moisture probe, UV sensor, solar charging circuit with Li-ion battery and BMS.\r\n- **Power strategy:** Duty-cycle the MCU and radio (wake, read, transmit, sleep) to minimize energy use; use the RTC to preserve timestamps across deep sleep.\r\n- **Connectivity & backend:** Wi-Fi to a local router; Node.js/Express API writing to MongoDB for time-series logging and an interactive React + D3 dashboard.\r\n\r\n## Key Technical Decisions\r\n\r\n- Reliable timestamps are essential: the firmware compares RTC time vs. expected wake intervals and marks/backfills missing samples on reconnect to preserve continuity.\r\n- Duty cycling and conservative sensor polling (hourly by default) balanced data fidelity and battery life; the power system was sized to survive multi-day cloudy periods.\r\n- Backend uses a document store (MongoDB) to allow flexible, schema-light time-series logging and easy prototyping of front-end visualizations.\r\n\r\n## Evaluation & Results\r\n\r\n- **Field test (30 days):** The station recorded hourly readings with no downtime; battery remained in safe operating range through cloudy intervals.\r\n- **Sensor accuracy:** Compared to a lab reference, the BME280 readings were within acceptable error bounds (within about 5% for humidity/temperature in the test period).\r\n- **Web access:** Dashboard successfully retrieved hourly updates and supported historical queries and visualization.\r\n\r\n(High-level results summarized from project notes and course write-up.) :contentReference[oaicite:4]{index=4}\r\n\r\n## My Contribution\r\n\r\n- Designed the power system and duty-cycling strategy for reliable off-grid operation.\r\n- Wrote ESP32 firmware (Arduino/C++) to handle sensor reads, RTC-based wake/sleep, and robust transmission with reconnect/backfill logic.\r\n- Built the backend API and the React/D3 dashboard for live and historical visualization.\r\n- Led the field deployment and validation tests.\r\n\r\n## Outcomes & Next Steps\r\n\r\n- Opuntia demonstrates practical autonomy for sensor-driven installations and ecological art. Next steps include adding OTA updates, encrypted telemetry, and optional cellular failover for remote deployments.\r\n",
          "code": "var Component=(()=>{var u=Object.create;var o=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var i in e)o(t,i,{get:e[i],enumerable:!0})},s=(t,e,i,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!y.call(t,r)&&r!==i&&o(t,r,{get:()=>e[r],enumerable:!(a=m(e,r))||a.enumerable});return t};var v=(t,e,i)=>(i=t!=null?u(g(t)):{},s(e||!t||!t.__esModule?o(i,\"default\",{value:t,enumerable:!0}):i,t)),w=t=>s(o({},\"__esModule\",{value:!0}),t);var d=b((D,l)=>{l.exports=_jsx_runtime});var k={};f(k,{default:()=>h,frontmatter:()=>x});var n=v(d()),x={title:\"Opuntia \\u2013 Solar-Powered Environmental Sensing Station\",slug:\"opuntia\",kind:\"research\",year:\"2024\",role:\"Designer & builder \\u2013 hardware, sensing, and logging\",themes:[\"Embodied Perception & Sensor Fusion\"],tags:[\"IoT\",\"ESP32-C3\",\"environmental-sensing\",\"solar-power\"],heroMetric:\"Solar-powered \\xB7 multi-sensor telemetry\",hook:\"A long-running environmental station for logging and visualizing environmental change.\",featured:!1,order:3,links:{github:\"https://github.com/opuntia-cc\"}};function c(t){let e={h2:\"h2\",li:\"li\",p:\"p\",strong:\"strong\",ul:\"ul\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Problem & Motivation\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Long-term ecological sensing and sensor-driven art installations are often constrained by power, communications reliability, and timestamp integrity in outdoor deployments. Opuntia was built to demonstrate that a compact, solar-powered station can reliably collect, backfill, and serve environmental telemetry (temperature, humidity, pressure, soil moisture, UV) to a web dashboard for weeks without human intervention. This project was developed during an IoT course as a practical research prototype. :contentReference[oaicite:3]\",index=3]}),`\n`,(0,n.jsx)(e.h2,{children:\"Approach & System Overview\"}),`\n`,(0,n.jsx)(e.p,{children:\"Opuntia emphasizes robustness and autonomy rather than experimental novelty: hardware and software are designed around a strict power budget, reliable timekeeping, and outage recovery.\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Hardware:\"}),\" ESP32-C3 microcontroller, BME280 (temp/humidity/pressure), soil-moisture probe, UV sensor, solar charging circuit with Li-ion battery and BMS.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Power strategy:\"}),\" Duty-cycle the MCU and radio (wake, read, transmit, sleep) to minimize energy use; use the RTC to preserve timestamps across deep sleep.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Connectivity & backend:\"}),\" Wi-Fi to a local router; Node.js/Express API writing to MongoDB for time-series logging and an interactive React + D3 dashboard.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Key Technical Decisions\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Reliable timestamps are essential: the firmware compares RTC time vs. expected wake intervals and marks/backfills missing samples on reconnect to preserve continuity.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Duty cycling and conservative sensor polling (hourly by default) balanced data fidelity and battery life; the power system was sized to survive multi-day cloudy periods.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Backend uses a document store (MongoDB) to allow flexible, schema-light time-series logging and easy prototyping of front-end visualizations.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Evaluation & Results\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Field test (30 days):\"}),\" The station recorded hourly readings with no downtime; battery remained in safe operating range through cloudy intervals.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Sensor accuracy:\"}),\" Compared to a lab reference, the BME280 readings were within acceptable error bounds (within about 5% for humidity/temperature in the test period).\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Web access:\"}),\" Dashboard successfully retrieved hourly updates and supported historical queries and visualization.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"(High-level results summarized from project notes and course write-up.) :contentReference[oaicite:4]\",index=4]}),`\n`,(0,n.jsx)(e.h2,{children:\"My Contribution\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Designed the power system and duty-cycling strategy for reliable off-grid operation.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Wrote ESP32 firmware (Arduino/C++) to handle sensor reads, RTC-based wake/sleep, and robust transmission with reconnect/backfill logic.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Built the backend API and the React/D3 dashboard for live and historical visualization.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Led the field deployment and validation tests.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Outcomes & Next Steps\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Opuntia demonstrates practical autonomy for sensor-driven installations and ecological art. Next steps include adding OTA updates, encrypted telemetry, and optional cellular failover for remote deployments.\"}),`\n`]})]})}function h(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(c,{...t})}):c(t)}return w(k);})();\n;return Component;"
        },
        "_id": "projects/opuntia.mdx",
        "_raw": {
          "sourceFilePath": "projects/opuntia.mdx",
          "sourceFileName": "opuntia.mdx",
          "sourceFileDir": "projects",
          "contentType": "mdx",
          "flattenedPath": "projects/opuntia"
        },
        "type": "Project",
        "url": "/projects/opuntia"
      },
      "documentHash": "1767592073116",
      "hasWarnings": false,
      "documentTypeName": "Project"
    },
    "projects/tft-finance.mdx": {
      "document": {
        "title": "Temporal Fusion Transformers for Financial Forecasting",
        "slug": "tft-finance",
        "kind": "experiment",
        "year": "2024",
        "role": "Co-lead – modeling and data pipeline",
        "themes": [
          "Temporal Models & Sequences"
        ],
        "tags": [
          "time-series",
          "temporal-fusion-transformer",
          "finance",
          "deep-learning"
        ],
        "heroMetric": "Exploratory forecasting on S&P 500 and related series",
        "hook": "Experiments with Temporal Fusion Transformers on financial time-series data.",
        "featured": false,
        "order": 5,
        "links": {
          "github": "https://github.com/samufrank/sp500-tft-forecasting",
          "type": "ProjectLinks",
          "_raw": {}
        },
        "body": {
          "raw": "\r\n\r\n## Project Context\r\n\r\nDeveloped as the final project for a graduate Deep Learning course, this effort applied Temporal Fusion Transformers (TFT) to S&P 500 forecasting using a mixed-frequency pipeline that combined daily price data with lower-frequency macroeconomic indicators. The goal was to investigate whether TFT's attention and gating mechanisms improve multi-horizon forecasting relative to LSTM baselines and to explore interpretability via attention weights. \r\n\r\n## Approach & System Overview\r\n\r\n- **Data pipeline:** Collected daily S&P prices (Yahoo Finance) and monthly/quarterly macro indicators (FRED). Designed alignment and imputation to handle mixed frequencies and avoid lookahead bias.\r\n- **Models implemented:** TFT (primary) and LSTM (baseline) implemented in PyTorch Lightning; training on GPU with time-series cross-validation splits.\r\n- **Architectural twist:** Experimented with separate embeddings for endogenous (price series) vs. exogenous (macro indicators) inputs to improve feature specialization.\r\n\r\n## Key Technical Points\r\n\r\n- Carefully handled time alignment: for each forecast horizon we limited use of \"known future\" features to only those legitimately available at forecast time.\r\n- Used attention weight visualizations to surface which inputs the model relied on for different horizons (e.g., interest rates for mid-horizon).\r\n- Logged experiments and results with TensorBoard and structured model checkpoints.\r\n\r\n## Results & Takeaways\r\n\r\n- TFT provided modest improvements (~5% RMSE improvement over tuned LSTM baselines) in the tested windows; TFT training incurred higher compute/time costs but offered improved interpretability via attention heatmaps.\r\n- Ablation showed that removing macroeconomic exogenous features increased error noticeably (~~10% in some splits), suggesting their value for multi-horizon forecasting.\r\n- These results are course-level exploratory findings and illustrate competence with modern temporal architectures and disciplined experimental practice in constrained timelines. :contentReference[oaicite:8]{index=8}\r\n\r\n## My Contribution\r\n\r\n- Led the data engineering: multi-source ingestion, alignment, and preprocessing.\r\n- Implemented the TFT and LSTM training loops, evaluation metrics, and ablation experiments.\r\n- Produced attention visualizations and draft report/presentation for the course.\r\n\r\n## Next Steps\r\n\r\n- Extend the setup to include higher-frequency real-time indicators, perform more robust cross-validation, and explore physically informed priors for improved generalization in volatile regimes.\r\n",
          "code": "var Component=(()=>{var m=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var T=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),v=(i,e)=>{for(var t in e)o(i,t,{get:e[t],enumerable:!0})},s=(i,e,t,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!f.call(i,r)&&r!==t&&o(i,r,{get:()=>e[r],enumerable:!(a=p(e,r))||a.enumerable});return i};var x=(i,e,t)=>(t=i!=null?m(g(i)):{},s(e||!i||!i.__esModule?o(t,\"default\",{value:i,enumerable:!0}):t,i)),y=i=>s(o({},\"__esModule\",{value:!0}),i);var d=T((M,l)=>{l.exports=_jsx_runtime});var b={};v(b,{default:()=>h,frontmatter:()=>w});var n=x(d()),w={title:\"Temporal Fusion Transformers for Financial Forecasting\",slug:\"tft-finance\",kind:\"experiment\",year:\"2024\",role:\"Co-lead \\u2013 modeling and data pipeline\",themes:[\"Temporal Models & Sequences\"],tags:[\"time-series\",\"temporal-fusion-transformer\",\"finance\",\"deep-learning\"],heroMetric:\"Exploratory forecasting on S&P 500 and related series\",hook:\"Experiments with Temporal Fusion Transformers on financial time-series data.\",featured:!1,order:5,links:{github:\"https://github.com/samufrank/sp500-tft-forecasting\"}};function c(i){let e={h2:\"h2\",li:\"li\",p:\"p\",strong:\"strong\",ul:\"ul\",...i.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Project Context\"}),`\n`,(0,n.jsx)(e.p,{children:\"Developed as the final project for a graduate Deep Learning course, this effort applied Temporal Fusion Transformers (TFT) to S&P 500 forecasting using a mixed-frequency pipeline that combined daily price data with lower-frequency macroeconomic indicators. The goal was to investigate whether TFT's attention and gating mechanisms improve multi-horizon forecasting relative to LSTM baselines and to explore interpretability via attention weights.\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Approach & System Overview\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Data pipeline:\"}),\" Collected daily S&P prices (Yahoo Finance) and monthly/quarterly macro indicators (FRED). Designed alignment and imputation to handle mixed frequencies and avoid lookahead bias.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Models implemented:\"}),\" TFT (primary) and LSTM (baseline) implemented in PyTorch Lightning; training on GPU with time-series cross-validation splits.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Architectural twist:\"}),\" Experimented with separate embeddings for endogenous (price series) vs. exogenous (macro indicators) inputs to improve feature specialization.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Key Technical Points\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:'Carefully handled time alignment: for each forecast horizon we limited use of \"known future\" features to only those legitimately available at forecast time.'}),`\n`,(0,n.jsx)(e.li,{children:\"Used attention weight visualizations to surface which inputs the model relied on for different horizons (e.g., interest rates for mid-horizon).\"}),`\n`,(0,n.jsx)(e.li,{children:\"Logged experiments and results with TensorBoard and structured model checkpoints.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Results & Takeaways\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"TFT provided modest improvements (~5% RMSE improvement over tuned LSTM baselines) in the tested windows; TFT training incurred higher compute/time costs but offered improved interpretability via attention heatmaps.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Ablation showed that removing macroeconomic exogenous features increased error noticeably (~~10% in some splits), suggesting their value for multi-horizon forecasting.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"These results are course-level exploratory findings and illustrate competence with modern temporal architectures and disciplined experimental practice in constrained timelines. :contentReference[oaicite:8]\",index=8]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"My Contribution\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Led the data engineering: multi-source ingestion, alignment, and preprocessing.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Implemented the TFT and LSTM training loops, evaluation metrics, and ablation experiments.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Produced attention visualizations and draft report/presentation for the course.\"}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Next Steps\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Extend the setup to include higher-frequency real-time indicators, perform more robust cross-validation, and explore physically informed priors for improved generalization in volatile regimes.\"}),`\n`]})]})}function h(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,{...i,children:(0,n.jsx)(c,{...i})}):c(i)}return y(b);})();\n;return Component;"
        },
        "_id": "projects/tft-finance.mdx",
        "_raw": {
          "sourceFilePath": "projects/tft-finance.mdx",
          "sourceFileName": "tft-finance.mdx",
          "sourceFileDir": "projects",
          "contentType": "mdx",
          "flattenedPath": "projects/tft-finance"
        },
        "type": "Project",
        "url": "/projects/tft-finance"
      },
      "documentHash": "1767592078200",
      "hasWarnings": false,
      "documentTypeName": "Project"
    }
  }
}
